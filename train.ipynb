{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962652cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # ðŸŠ Pelatihan Klasifikasi Citra Jeruk dengan EfficientNetV2-L (Optimized & Checkpointed)\n",
    "# \n",
    "# ## Sel 1: Setup - Import, Wrapper, dan Fungsi Training\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# region Sel 1: Import, Wrapper, dan Definisi Fungsi\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset, WeightedRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import EfficientNet_V2_L_Weights\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*is still alive.*\")\n",
    "\n",
    "class CheckpointWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self.module, x, use_reentrant=False)\n",
    "\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, device, num_epochs, dataset_sizes, phase_name=\"Training\", scaler=None): \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    print(f\"\\n--- Memulai {phase_name} (Epochs: {num_epochs}, AMP: {'Aktif' if scaler else 'Nonaktif'}) ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            dataloader = dataloaders[phase]\n",
    "            pbar = tqdm(dataloader, desc=f\"{phase.capitalize()} Epoch {epoch+1}\", leave=False)\n",
    "\n",
    "            for inputs, labels in pbar:\n",
    "                try:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError memindahkan data ke {device}: {e}\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'), torch.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type == 'cuda' and scaler is not None)): \n",
    "                    try:\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    except Exception as e:\n",
    "                         print(f\"\\nError during forward pass (Autocast): {e}\", file=sys.stderr)\n",
    "                         if \"CUDA out of memory\" in str(e):\n",
    "                             print(\" (!) CUDA out of memory. Coba kurangi BATCH_SIZE_TUNE.\", file=sys.stderr)\n",
    "                         continue\n",
    "\n",
    "                if phase == 'train':\n",
    "                    if scaler:\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{torch.sum(preds == labels.data)/inputs.size(0):.4f}'})\n",
    "\n",
    "            if dataset_sizes[phase] == 0:\n",
    "                 epoch_loss, epoch_acc = 0.0, 0.0\n",
    "            else:\n",
    "                 epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                 epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f\"  {phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    print(f\"    -> Akurasi validasi terbaik baru: {best_acc:.4f}. Menyimpan model sementara...\")\n",
    "                    torch.save(model.state_dict(), f'best_model_{phase_name.replace(\" \", \"_\").lower()}_temp.pth')\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            print(f\"  LR disetel ke: {scheduler.get_last_lr()[0]:.1e}\")\n",
    "\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"{phase_name} selesai dalam {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Akurasi Validasi Terbaik: {best_acc:.4f}\")\n",
    "    \n",
    "    print(\"Memuat bobot model terbaik...\")\n",
    "    try:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model weights: {e}\", file=sys.stderr)\n",
    "    return model, history\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Sel 2: Konfigurasi Parameter dan Persiapan Data (Final Revision)\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# region Sel 2: Konfigurasi dan Persiapan Data\n",
    "DATASET_PATH = 'Dataset'\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE_EXTRACT = 128\n",
    "BATCH_SIZE_TUNE = 16\n",
    "    \n",
    "EPOCHS_FEATURE_EXTRACT = 5\n",
    "EPOCHS_FINE_TUNE = 15\n",
    "    \n",
    "VALIDATION_SPLIT = 0.2\n",
    "LR_FEATURE_EXTRACT = 1e-3\n",
    "LR_FINE_TUNE = 1e-4\n",
    "    \n",
    "WEIGHT_DECAY = 1e-2\n",
    "LABEL_SMOOTHING = 0.1\n",
    "    \n",
    "PLOT_FILENAME = 'training_results_optimized.png'\n",
    "CONFUSION_MATRIX_FILENAME = 'confusion_matrix_optimized.png'\n",
    "BEST_MODEL_EXTRACT_PATH = 'best_model_extract_optimized.pth'\n",
    "MODEL_SAVE_PATH = 'citrus_efficientnetv2l_pytorch_final_checkpointed_optimized.pth'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Menggunakan Device: {device}\")\n",
    "\n",
    "print(\"Mempersiapkan transformasi data (Augmentasi Dihapus)...\")\n",
    "weights = EfficientNet_V2_L_Weights.DEFAULT\n",
    "preprocess = weights.transforms(antialias=True)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE + 32),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    preprocess\n",
    "])\n",
    "\n",
    "val_transforms = preprocess\n",
    "\n",
    "try:\n",
    "    full_dataset = datasets.ImageFolder(DATASET_PATH, transform=None)\n",
    "    CLASSES = sorted(full_dataset.classes)\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    print(f\"Total gambar: {len(full_dataset)}. Kelas: {NUM_CLASSES}\")\n",
    "\n",
    "    class_counts_dict = {full_dataset.classes[i]: count for i, count in enumerate(np.bincount(full_dataset.targets))}\n",
    "    counts = list(class_counts_dict.values())\n",
    "    is_imbalanced = min(counts) > 0 and (max(counts) / min(counts)) > 2.0 if counts else False\n",
    "    if is_imbalanced:\n",
    "         print(f\"(!) Imbalanced data terdeteksi ({class_counts_dict}). Mengaktifkan WeightedRandomSampler.\")\n",
    "\n",
    "    class_weights = [len(full_dataset) / class_counts_dict[full_dataset.classes[i]] for i in range(NUM_CLASSES)]\n",
    "    \n",
    "    total_len = len(full_dataset)\n",
    "    val_len = int(total_len * VALIDATION_SPLIT)\n",
    "    train_len = total_len - val_len\n",
    "\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    train_indices, val_indices = random_split(range(total_len), [train_len, val_len], generator=generator)\n",
    "\n",
    "    original_train_subset = Subset(full_dataset, train_indices)\n",
    "    \n",
    "    train_sampler = None\n",
    "    shuffle_train = True\n",
    "    if is_imbalanced and train_len > 0:\n",
    "        train_sample_weights = [class_weights[full_dataset.targets[i]] for i in original_train_subset.indices]\n",
    "        train_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=torch.DoubleTensor(train_sample_weights),\n",
    "            num_samples=train_len,\n",
    "            replacement=True\n",
    "        )\n",
    "        shuffle_train = False\n",
    "\n",
    "    train_dataset = TransformedSubset(original_train_subset, train_transforms)\n",
    "    val_dataset = TransformedSubset(Subset(full_dataset, val_indices), val_transforms)\n",
    "\n",
    "    num_workers = 0 \n",
    "    persistent = False\n",
    "    print(f\"Menggunakan num_workers={num_workers} (Solusi Deadlock).\")\n",
    "    \n",
    "    pin_memory = torch.cuda.is_available()\n",
    "\n",
    "    dataloaders_extract = {\n",
    "        'train': DataLoader(train_dataset, batch_size=BATCH_SIZE_EXTRACT, sampler=train_sampler, shuffle=shuffle_train, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=persistent),\n",
    "        'val': DataLoader(val_dataset, batch_size=BATCH_SIZE_EXTRACT, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=persistent)\n",
    "    }\n",
    "    dataloaders_tune = {\n",
    "        'train': DataLoader(train_dataset, batch_size=BATCH_SIZE_TUNE, sampler=train_sampler, shuffle=shuffle_train, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=persistent),\n",
    "        'val': DataLoader(val_dataset, batch_size=BATCH_SIZE_TUNE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=persistent)\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "    class_names = CLASSES\n",
    "    print(\"Dataset dan Dataloader siap.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saat memuat data: {e}\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9acedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Sel 3: Tahap 1: Feature Extraction\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# region Sel 3: Feature Extraction\n",
    "print(\"\\n=== TAHAP 1: FEATURE EXTRACTION (DENGAN AMP) ===\")\n",
    "model_extract = models.efficientnet_v2_l(weights=weights)\n",
    "\n",
    "print(\"(!) Menerapkan Gradient Checkpointing...\")\n",
    "for i, block_sequence in enumerate(model_extract.features):\n",
    "    if isinstance(block_sequence, nn.Sequential):\n",
    "        model_extract.features[i] = CheckpointWrapper(block_sequence)\n",
    "\n",
    "print(\"Membekukan bobot base model...\")\n",
    "for param in model_extract.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_extract.classifier[1].in_features\n",
    "model_extract.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.4, inplace=True),\n",
    "    nn.Linear(num_ftrs, NUM_CLASSES)\n",
    ")\n",
    "model_extract = model_extract.to(device)\n",
    "\n",
    "criterion_extract = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer_extract = optim.AdamW(model_extract.classifier.parameters(), lr=LR_FEATURE_EXTRACT, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_extract = optim.lr_scheduler.CosineAnnealingLR(optimizer_extract, T_max=EPOCHS_FEATURE_EXTRACT)\n",
    "\n",
    "# Sintaks diperbarui untuk PyTorch versi yang lebih baru\n",
    "scaler_extract = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None\n",
    "\n",
    "model_extract, history_extract = train_model(\n",
    "    model=model_extract,\n",
    "    criterion=criterion_extract,\n",
    "    optimizer=optimizer_extract,\n",
    "    scheduler=scheduler_extract, \n",
    "    dataloaders=dataloaders_extract,\n",
    "    device=device,\n",
    "    num_epochs=EPOCHS_FEATURE_EXTRACT,\n",
    "    dataset_sizes=dataset_sizes,\n",
    "    phase_name=\"Feature Extraction\",\n",
    "    scaler=scaler_extract\n",
    ")\n",
    "\n",
    "torch.save(model_extract.state_dict(), BEST_MODEL_EXTRACT_PATH)\n",
    "print(f\"Model Feature Extraction disimpan sebagai {BEST_MODEL_EXTRACT_PATH}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Sel 4: Tahap 2: Fine-Tuning\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# region Sel 4: Fine-Tuning\n",
    "print(\"\\n=== TAHAP 2: FINE-TUNING (DENGAN AMP) ===\")\n",
    "model_tune = models.efficientnet_v2_l(weights=None) \n",
    "num_ftrs_tune = model_tune.classifier[1].in_features\n",
    "model_tune.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.4, inplace=True),\n",
    "    nn.Linear(num_ftrs_tune, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "print(\"(!) Menerapkan Gradient Checkpointing untuk Fine-Tuning...\")\n",
    "for i, block_sequence in enumerate(model_tune.features):\n",
    "    if isinstance(block_sequence, nn.Sequential):\n",
    "        model_tune.features[i] = CheckpointWrapper(block_sequence)\n",
    "\n",
    "try:\n",
    "    print(f\"Mencoba memuat bobot dari: {BEST_MODEL_EXTRACT_PATH}\")\n",
    "    model_tune.load_state_dict(torch.load(BEST_MODEL_EXTRACT_PATH, map_location=torch.device('cpu')))\n",
    "    print(\"Bobot terbaik dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saat memuat bobot: {e}. Menggunakan state dari model_extract.\", file=sys.stderr)\n",
    "    model_tune.load_state_dict(model_extract.state_dict() if 'model_extract' in locals() else torch.load(f'best_model_feature_extraction_temp.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "print(\"Membuka semua bobot model...\")\n",
    "for param in model_tune.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_tune = model_tune.to(device)\n",
    "\n",
    "criterion_tune = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer_tune = optim.AdamW(model_tune.parameters(), lr=LR_FINE_TUNE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_tune = optim.lr_scheduler.CosineAnnealingLR(optimizer_tune, T_max=EPOCHS_FINE_TUNE)\n",
    "\n",
    "# Sintaks diperbarui untuk PyTorch versi yang lebih baru\n",
    "scaler_tune = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None\n",
    "\n",
    "model_final, history_tune = train_model(\n",
    "    model=model_tune,\n",
    "    criterion=criterion_tune,\n",
    "    optimizer=optimizer_tune,\n",
    "    scheduler=scheduler_tune, \n",
    "    dataloaders=dataloaders_tune,\n",
    "    device=device,\n",
    "    num_epochs=EPOCHS_FINE_TUNE,\n",
    "    dataset_sizes=dataset_sizes,\n",
    "    phase_name=\"Fine-Tuning\",\n",
    "    scaler=scaler_tune\n",
    ")\n",
    "\n",
    "combined_history = {}\n",
    "if 'history_extract' in locals():\n",
    "    combined_history['train_loss'] = history_extract.get('train_loss', []) + history_tune.get('train_loss', [])\n",
    "    combined_history['train_acc'] = history_extract.get('train_acc', []) + history_tune.get('train_acc', [])\n",
    "    combined_history['val_loss'] = history_extract.get('val_loss', []) + history_tune.get('val_loss', [])\n",
    "    combined_history['val_acc'] = history_extract.get('val_acc', []) + history_tune.get('val_acc', [])\n",
    "\n",
    "torch.save(model_final.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model final disimpan sebagai {MODEL_SAVE_PATH}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Sel 5: Visualisasi dan Evaluasi Hasil\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# region Sel 5: Visualisasi dan Evaluasi\n",
    "print(f\"\\nMembuat plot hasil training ({PLOT_FILENAME})...\")\n",
    "if combined_history and combined_history.get('train_acc'):\n",
    "    acc = combined_history['train_acc']\n",
    "    val_acc = combined_history['val_acc']\n",
    "    loss = combined_history['train_loss']\n",
    "    val_loss = combined_history['val_loss']\n",
    "    total_epochs_recorded = len(acc)\n",
    "    epochs_range = range(1, total_epochs_recorded + 1)\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy', marker='.', linestyle='-')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='.', linestyle='-')\n",
    "    if 'history_extract' in locals() and history_extract.get('train_acc'):\n",
    "         plt.axvline(len(history_extract['train_acc']) + 0.5, color='grey', linestyle='--', label='Start Fine-Tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss', marker='.', linestyle='-')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss', marker='.', linestyle='-')\n",
    "    if 'history_extract' in locals() and history_extract.get('train_acc'):\n",
    "         plt.axvline(len(history_extract['train_acc']) + 0.5, color='grey', linestyle='--', label='Start Fine-Tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.suptitle(f'Hasil Training EfficientNetV2-L ({total_epochs_recorded} Epochs)')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(PLOT_FILENAME)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"Plot disimpan sebagai {PLOT_FILENAME}\")\n",
    "else:\n",
    "    print(\"History tidak lengkap, plot tidak dibuat.\")\n",
    "\n",
    "print(f\"\\nMembuat Confusion Matrix ({CONFUSION_MATRIX_FILENAME})...\")\n",
    "if 'model_final' in locals() and isinstance(model_final, nn.Module):\n",
    "     model_to_eval = model_final\n",
    "     model_to_eval.eval()\n",
    "     all_preds = []\n",
    "     all_labels = []\n",
    "     eval_dataloader = dataloaders_tune['val']\n",
    "\n",
    "     with torch.no_grad():\n",
    "         for inputs, labels in tqdm(eval_dataloader, desc=\"Evaluasi Validasi\"):\n",
    "             inputs, labels = inputs.to(device), labels.to(device)\n",
    "             \n",
    "             with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type == 'cuda')):\n",
    "                outputs = model_to_eval(inputs)\n",
    "                \n",
    "             _, preds = torch.max(outputs, 1)\n",
    "             all_preds.extend(preds.cpu().numpy())\n",
    "             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "     cf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "     plt.figure(figsize=(10, 8))\n",
    "     sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                 xticklabels=class_names, yticklabels=class_names)\n",
    "     plt.xlabel('Predicted Label')\n",
    "     plt.ylabel('True Label')\n",
    "     plt.title('Confusion Matrix pada Data Validasi')\n",
    "     plt.xticks(rotation=45, ha='right')\n",
    "     plt.yticks(rotation=0)\n",
    "     plt.tight_layout()\n",
    "     plt.savefig(CONFUSION_MATRIX_FILENAME)\n",
    "     plt.show()\n",
    "     plt.close()\n",
    "     print(f\"Confusion Matrix disimpan sebagai {CONFUSION_MATRIX_FILENAME}\")\n",
    "else:\n",
    "     print(\"Model final tidak ditemukan, evaluasi dilewati.\")\n",
    "\n",
    "print(\"\\n--- Selesai ---\")\n",
    "# endregion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
