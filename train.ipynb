{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962652cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 1. Setup & Training Function\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset, WeightedRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import EfficientNet_V2_S_Weights\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)} | Prec: {AMP_DTYPE}\")\n",
    "\n",
    "class CheckpointWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self.module, x, use_reentrant=False)\n",
    "\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform: x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, device, num_epochs, dataset_sizes, phase_name=\"Training\", accumulation_steps=1): \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    print(f\"\\n--- {phase_name} ({num_epochs} Epochs) ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} [LR: {current_lr:.1e}]\", end=\"\")\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device, memory_format=torch.channels_last, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'), \\\n",
    "                     torch.autocast(device_type=device.type, dtype=AMP_DTYPE, enabled=True): \n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    continue\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss = loss / accumulation_steps\n",
    "                    loss.backward()\n",
    "                    if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloaders[phase]):\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                loss_val = loss.item() * accumulation_steps if phase == 'train' else loss.item()\n",
    "                running_loss += loss_val * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                print(f\" | T-Loss: {epoch_loss:.3f} Acc: {epoch_acc:.3f}\", end=\"\")\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                print(f\" | V-Loss: {epoch_loss:.3f} Acc: {epoch_acc:.3f}\")\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    try: torch.save(model.state_dict(), f'best_{phase_name.lower()[:4]}.pth')\n",
    "                    except: pass\n",
    "        \n",
    "        if scheduler: scheduler.step()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Done in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s. Best Acc: {best_acc:.4f}\")\n",
    "    try: model.load_state_dict(best_model_wts)\n",
    "    except: pass\n",
    "    return model, history\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 2. Config & Data Loading\n",
    "DATASET_PATH = 'Dataset'\n",
    "IMG_SIZE = 224\n",
    "\n",
    "RUN_FEATURE_EXTRACTION = True  \n",
    "RUN_FINE_TUNING = True         \n",
    "\n",
    "MICRO_BATCH_SIZE = 8\n",
    "ACCUM_STEPS_EXTRACT = 4 \n",
    "ACCUM_STEPS_TUNE = 4    \n",
    "    \n",
    "EPOCHS_FEATURE_EXTRACT = 20\n",
    "EPOCHS_FINE_TUNE = 30\n",
    "    \n",
    "VALIDATION_SPLIT = 0.2 \n",
    "LR_FEATURE_EXTRACT = 1e-2 \n",
    "LR_FINE_TUNE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTHING = 0.1\n",
    "    \n",
    "PLOT_FILENAME = 'training_results.png'\n",
    "CONF_MATRIX_FILE = 'confusion_matrix.png'\n",
    "BEST_FE_PATH = 'best_feat.pth'\n",
    "FINAL_MODEL_PATH = 'citrus_v2s_final.pth'\n",
    "\n",
    "def prepare_data():\n",
    "    weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "    preprocess = weights.transforms(antialias=True)\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE + 32),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        preprocess\n",
    "    ])\n",
    "    val_transforms = preprocess\n",
    "\n",
    "    try:\n",
    "        full_dataset = datasets.ImageFolder(DATASET_PATH, transform=None)\n",
    "        CLASSES = sorted(full_dataset.classes)\n",
    "        NUM_CLASSES = len(CLASSES)\n",
    "        \n",
    "        print(f\"Data: {len(full_dataset)} imgs | Classes: {NUM_CLASSES}\")\n",
    "        print(f\"Batch: {MICRO_BATCH_SIZE} (Accum: {ACCUM_STEPS_EXTRACT}/{ACCUM_STEPS_TUNE})\")\n",
    "\n",
    "        class_counts = np.bincount(full_dataset.targets)\n",
    "        class_weights = [len(full_dataset) / c for c in class_counts]\n",
    "        \n",
    "        train_size = int((1 - VALIDATION_SPLIT) * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        train_indices, val_indices = random_split(range(len(full_dataset)), [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        train_targets = [full_dataset.targets[i] for i in train_indices]\n",
    "        sample_weights = [class_weights[t] for t in train_targets]\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train_indices), replacement=True)\n",
    "\n",
    "        train_dataset = TransformedSubset(Subset(full_dataset, train_indices), train_transforms)\n",
    "        val_dataset = TransformedSubset(Subset(full_dataset, val_indices), val_transforms)\n",
    "        \n",
    "        dataloaders_extract = {\n",
    "            'train': DataLoader(train_dataset, batch_size=MICRO_BATCH_SIZE, sampler=sampler, num_workers=0, pin_memory=True),\n",
    "            'val': DataLoader(val_dataset, batch_size=MICRO_BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        }\n",
    "        dataloaders_tune = {\n",
    "            'train': DataLoader(train_dataset, batch_size=MICRO_BATCH_SIZE, sampler=sampler, num_workers=0, pin_memory=True),\n",
    "            'val': DataLoader(val_dataset, batch_size=MICRO_BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        }\n",
    "        dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "        return dataloaders_extract, dataloaders_tune, dataset_sizes, CLASSES, NUM_CLASSES, weights\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dl_extract, dl_tune, ds_sizes, class_names, num_classes, weights = prepare_data()\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9acedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 3. Feature Extraction\n",
    "if __name__ == '__main__':\n",
    "    if 'dl_extract' not in locals():\n",
    "        dl_extract, dl_tune, ds_sizes, class_names, num_classes, weights = prepare_data()\n",
    "\n",
    "    if RUN_FEATURE_EXTRACTION:\n",
    "        print(\"\\n>>> START FEATURE EXTRACTION\")\n",
    "        model = models.efficientnet_v2_s(weights=weights)\n",
    "        \n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for i in range(len(model.features)):\n",
    "            model.features[i] = CheckpointWrapper(model.features[i])\n",
    "\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4, inplace=True),\n",
    "            nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        )\n",
    "        model = model.to(DEVICE, memory_format=torch.channels_last)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "        optimizer = optim.SGD(model.classifier.parameters(), lr=LR_FEATURE_EXTRACT, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FEATURE_EXTRACT)\n",
    "\n",
    "        model, hist_extract = train_model(\n",
    "            model, criterion, optimizer, scheduler, dl_extract, DEVICE, \n",
    "            EPOCHS_FEATURE_EXTRACT, ds_sizes, \"FeatExtract\", ACCUM_STEPS_EXTRACT\n",
    "        )\n",
    "        torch.save(model.state_dict(), BEST_FE_PATH)\n",
    "        \n",
    "        del model, optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"\\n>>> FEATURE EXTRACTION SKIPPED\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 4. Fine Tuning\n",
    "if __name__ == '__main__':\n",
    "    if RUN_FINE_TUNING:\n",
    "        print(\"\\n>>> START FINE TUNING\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if 'dl_tune' not in locals():\n",
    "             dl_extract, dl_tune, ds_sizes, class_names, num_classes, weights = prepare_data()\n",
    "             \n",
    "        model = models.efficientnet_v2_s(weights=None) \n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4, inplace=True),\n",
    "            nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        for i in range(len(model.features)):\n",
    "            model.features[i] = CheckpointWrapper(model.features[i])\n",
    "\n",
    "        if os.path.exists(BEST_FE_PATH):\n",
    "            model.load_state_dict(torch.load(BEST_FE_PATH, map_location='cpu'))\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        model = model.to(DEVICE, memory_format=torch.channels_last)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LR_FINE_TUNE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FINE_TUNE)\n",
    "\n",
    "        model, hist_tune = train_model(\n",
    "            model, criterion, optimizer, scheduler, dl_tune, DEVICE, \n",
    "            EPOCHS_FINE_TUNE, ds_sizes, \"FineTuning\", ACCUM_STEPS_TUNE\n",
    "        )\n",
    "        torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "    else:\n",
    "        print(\"\\n>>> FINE TUNING SKIPPED\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 5. Visualization\n",
    "from sklearn.metrics import classification_report\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n>>> VISUALIZATION\")\n",
    "    \n",
    "    if 'hist_extract' in locals() or 'hist_tune' in locals():\n",
    "        acc = hist_extract.get('val_acc', []) + hist_tune.get('val_acc', []) if 'hist_extract' in locals() else hist_tune.get('val_acc', [])\n",
    "        loss = hist_extract.get('val_loss', []) + hist_tune.get('val_loss', []) if 'hist_extract' in locals() else hist_tune.get('val_loss', [])\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1); plt.plot(acc); plt.title('Acc'); plt.grid(True, alpha=0.3)\n",
    "        plt.subplot(1, 2, 2); plt.plot(loss, color='red'); plt.title('Loss'); plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(PLOT_FILENAME); plt.show()\n",
    "\n",
    "    if 'model' in locals():\n",
    "        model.eval()\n",
    "        preds, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(dl_tune['val'], desc=\"Eval\"):\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                with torch.autocast(device_type=DEVICE.type, dtype=torch.bfloat16, enabled=True):\n",
    "                    out = model(x)\n",
    "                preds.extend(torch.max(out, 1)[1].cpu().numpy())\n",
    "                labels.extend(y.cpu().numpy())\n",
    "\n",
    "        print(classification_report(labels, preds, target_names=class_names, digits=4))\n",
    "        \n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix'); plt.savefig(CONF_MATRIX_FILE); plt.show()\n",
    "# endregion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
