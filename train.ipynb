{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962652cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM INFO ===\n",
      "Device: NVIDIA GeForce RTX 5060\n",
      "Mode: BFloat16 (BF16) + Channels Last + TF32\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "# region 1. Setup & Training Function\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset, WeightedRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import EfficientNet_V2_L_Weights\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- HARDWARE OPTIMIZATION ---\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "if DEVICE.type == 'cuda':\n",
    "    # 1. cuDNN Benchmark (Auto-tuner)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # 2. TF32 (TensorFloat-32)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# 3. Force BFloat16\n",
    "if torch.cuda.is_bf16_supported() and DEVICE.type == 'cuda':\n",
    "    AMP_DTYPE = torch.bfloat16\n",
    "    PRECISION_NAME = \"BFloat16 (BF16) + Channels Last + TF32\"\n",
    "else:\n",
    "    raise RuntimeError(\"❌ GPU tidak support BF16!\")\n",
    "\n",
    "print(f\"=== SYSTEM INFO ===\\nDevice: {torch.cuda.get_device_name(0)}\\nMode: {PRECISION_NAME}\\n===================\")\n",
    "\n",
    "class CheckpointWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self.module, x, use_reentrant=False)\n",
    "\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform: x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, device, num_epochs, dataset_sizes, phase_name=\"Training\", val_interval=1): \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"\\n--- Memulai {phase_name} ({num_epochs} Epochs) ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        phases = ['train']\n",
    "        # Validasi interval untuk hemat waktu\n",
    "        if (epoch + 1) % val_interval == 0 or (epoch + 1) == num_epochs:\n",
    "            phases.append('val')\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for phase in phases:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            \n",
    "            pbar = tqdm(dataloaders[phase], desc=f\"{phase.capitalize()}\", leave=False)\n",
    "\n",
    "            for inputs, labels in pbar:\n",
    "                # OPTIMASI: Channels Last (NHWC)\n",
    "                inputs = inputs.to(device, memory_format=torch.channels_last, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # OPTIMASI: Zero grad None\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Autocast BF16\n",
    "                with torch.set_grad_enabled(phase == 'train'), \\\n",
    "                     torch.autocast(device_type=device.type, dtype=AMP_DTYPE, enabled=True): \n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{torch.sum(preds == labels.data)/inputs.size(0):.4f}'})\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print(f\"  {phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(model.state_dict(), f'best_model_{phase_name.replace(\" \", \"_\").lower()}_temp.pth')\n",
    "        \n",
    "        if scheduler: scheduler.step()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Selesai: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s | Best Acc: {best_acc:.4f}\")\n",
    "    try: model.load_state_dict(best_model_wts)\n",
    "    except: pass\n",
    "    return model, history\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9f337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 2. Config & Data Loading\n",
    "DATASET_PATH = 'Dataset'\n",
    "IMG_SIZE = 224\n",
    "\n",
    "BATCH_SIZE_EXTRACT = 64  \n",
    "BATCH_SIZE_TUNE = 8      \n",
    "    \n",
    "EPOCHS_FEATURE_EXTRACT = 10\n",
    "EPOCHS_FINE_TUNE = 20\n",
    "    \n",
    "VALIDATION_SPLIT = 0.2 \n",
    "LR_FEATURE_EXTRACT = 1e-2\n",
    "LR_FINE_TUNE = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "LABEL_SMOOTHING = 0.1\n",
    "    \n",
    "PLOT_FILENAME = 'training_results_final.png'\n",
    "CONFUSION_MATRIX_FILENAME = 'confusion_matrix_final.png'\n",
    "BEST_MODEL_EXTRACT_PATH = 'best_model_extract.pth'\n",
    "MODEL_SAVE_PATH = 'citrus_efficientnetv2l_final.pth'\n",
    "\n",
    "def prepare_data():\n",
    "    print(\"Mempersiapkan Data...\")\n",
    "    weights = EfficientNet_V2_L_Weights.DEFAULT\n",
    "    preprocess = weights.transforms(antialias=True)\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE + 32),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        preprocess\n",
    "    ])\n",
    "    val_transforms = preprocess\n",
    "\n",
    "    try:\n",
    "        full_dataset = datasets.ImageFolder(DATASET_PATH, transform=None)\n",
    "        CLASSES = sorted(full_dataset.classes)\n",
    "        NUM_CLASSES = len(CLASSES)\n",
    "        print(f\"Dataset: {len(full_dataset)} images | {NUM_CLASSES} classes\")\n",
    "\n",
    "        class_counts = np.bincount(full_dataset.targets)\n",
    "        class_weights = [len(full_dataset) / c for c in class_counts]\n",
    "        \n",
    "        train_size = int((1 - VALIDATION_SPLIT) * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        train_indices, val_indices = random_split(range(len(full_dataset)), [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        train_targets = [full_dataset.targets[i] for i in train_indices]\n",
    "        sample_weights = [class_weights[t] for t in train_targets]\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train_indices), replacement=True)\n",
    "\n",
    "        train_dataset = TransformedSubset(Subset(full_dataset, train_indices), train_transforms)\n",
    "        val_dataset = TransformedSubset(Subset(full_dataset, val_indices), val_transforms)\n",
    "\n",
    "        num_workers = 0 \n",
    "        \n",
    "        dataloaders_extract = {\n",
    "            'train': DataLoader(train_dataset, batch_size=BATCH_SIZE_EXTRACT, sampler=sampler, num_workers=num_workers, pin_memory=True),\n",
    "            'val': DataLoader(val_dataset, batch_size=BATCH_SIZE_EXTRACT, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        }\n",
    "        dataloaders_tune = {\n",
    "            'train': DataLoader(train_dataset, batch_size=BATCH_SIZE_TUNE, sampler=sampler, num_workers=num_workers, pin_memory=True),\n",
    "            'val': DataLoader(val_dataset, batch_size=BATCH_SIZE_TUNE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        }\n",
    "        dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "        \n",
    "        return dataloaders_extract, dataloaders_tune, dataset_sizes, CLASSES, NUM_CLASSES, weights\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error Dataset: {e}\")\n",
    "        sys.exit(1)\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9acedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TAHAP 1: FEATURE EXTRACTION (BF16 + Channels Last) ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== TAHAP 1: FEATURE EXTRACTION (BF16 + Channels Last) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. Load Pretrained Model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model_extract = models.efficientnet_v2_l(weights=\u001b[43mweights\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2. Gradient Checkpointing (Hemat VRAM)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model_extract.features)):\n",
      "\u001b[31mNameError\u001b[39m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "# region 3. Feature Extraction\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n=== TAHAP 1: FEATURE EXTRACTION (BF16 + Channels Last) ===\")\n",
    "    \n",
    "    # --- PERBAIKAN: Cek & Load Data Otomatis ---\n",
    "    # Memastikan variabel 'weights', 'num_classes', dll sudah tersedia\n",
    "    try:\n",
    "        # Cek apakah variabel penting sudah ada di memori\n",
    "        if 'weights' not in locals() and 'weights' not in globals():\n",
    "            raise NameError(\"weights not found\")\n",
    "        if 'dataloaders_extract' not in locals() and 'dataloaders_extract' not in globals():\n",
    "            raise NameError(\"dataloaders not found\")\n",
    "    except NameError:\n",
    "        print(\"(!) Data belum dimuat. Menjalankan prepare_data() otomatis...\")\n",
    "        try:\n",
    "            # Panggil fungsi dari Sel 2\n",
    "            dataloaders_extract, dataloaders_tune, dataset_sizes, class_names, num_classes, weights = prepare_data()\n",
    "            print(\"(!) Data berhasil dimuat.\")\n",
    "        except NameError:\n",
    "            raise RuntimeError(\"❌ Fungsi 'prepare_data' tidak ditemukan. Mohon jalankan Sel 2 terlebih dahulu!\")\n",
    "\n",
    "    # 1. Load Pretrained Model\n",
    "    model_extract = models.efficientnet_v2_l(weights=weights)\n",
    "\n",
    "    # 2. Gradient Checkpointing (Wajib untuk VRAM 8GB)\n",
    "    for i in range(len(model_extract.features)):\n",
    "        model_extract.features[i] = CheckpointWrapper(model_extract.features[i])\n",
    "\n",
    "    # 3. Freeze Backbone\n",
    "    for param in model_extract.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 4. Ganti Classifier Head\n",
    "    num_ftrs = model_extract.classifier[1].in_features\n",
    "    model_extract.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4, inplace=True),\n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    )\n",
    "    \n",
    "    # 5. Optimasi Memori: Channels Last\n",
    "    model_extract = model_extract.to(DEVICE, memory_format=torch.channels_last)\n",
    "\n",
    "    # Setup Training\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "    optimizer = optim.AdamW(model_extract.classifier.parameters(), lr=LR_FEATURE_EXTRACT, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FEATURE_EXTRACT)\n",
    "\n",
    "    # Jalankan Training (Validasi tiap epoch)\n",
    "    model_extract, history_extract = train_model(\n",
    "        model_extract, criterion, optimizer, scheduler, dataloaders_extract, DEVICE, \n",
    "        EPOCHS_FEATURE_EXTRACT, dataset_sizes, phase_name=\"Feature Extraction\", val_interval=1\n",
    "    )\n",
    "    \n",
    "    # Simpan Checkpoint\n",
    "    torch.save(model_extract.state_dict(), BEST_MODEL_EXTRACT_PATH)\n",
    "    print(f\"Checkpoint Feature Extraction tersimpan: {BEST_MODEL_EXTRACT_PATH}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 4. Fine Tuning\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n=== TAHAP 2: FINE-TUNING (BF16 + Channels Last) ===\")\n",
    "    \n",
    "    # 1. Re-Init Model\n",
    "    model_tune = models.efficientnet_v2_l(weights=None) \n",
    "    num_ftrs_tune = model_tune.classifier[1].in_features\n",
    "    model_tune.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4, inplace=True),\n",
    "        nn.Linear(num_ftrs_tune, num_classes)\n",
    "    )\n",
    "\n",
    "    # 2. Re-Apply Checkpointing\n",
    "    for i in range(len(model_tune.features)):\n",
    "        model_tune.features[i] = CheckpointWrapper(model_tune.features[i])\n",
    "\n",
    "    # 3. Load Bobot dari Feature Extraction\n",
    "    try:\n",
    "        model_tune.load_state_dict(torch.load(BEST_MODEL_EXTRACT_PATH, map_location='cpu'))\n",
    "        print(\"Bobot Feature Extraction dimuat.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal memuat bobot, menggunakan inisialisasi awal. Error: {e}\")\n",
    "        model_tune.load_state_dict(model_extract.state_dict())\n",
    "\n",
    "    # 4. Unfreeze Semua Layer\n",
    "    for param in model_tune.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # 5. Optimasi Memori: Channels Last\n",
    "    model_tune = model_tune.to(DEVICE, memory_format=torch.channels_last)\n",
    "\n",
    "    # Setup Training (LR lebih kecil)\n",
    "    optimizer_tune = optim.AdamW(model_tune.parameters(), lr=LR_FINE_TUNE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler_tune = optim.lr_scheduler.CosineAnnealingLR(optimizer_tune, T_max=EPOCHS_FINE_TUNE)\n",
    "\n",
    "    # Jalankan Training (Validasi setiap 2 epoch untuk kecepatan)\n",
    "    model_final, history_tune = train_model(\n",
    "        model_tune, criterion, optimizer_tune, scheduler_tune, dataloaders_tune, DEVICE, \n",
    "        EPOCHS_FINE_TUNE, dataset_sizes, phase_name=\"Fine-Tuning\", val_interval=2\n",
    "    )\n",
    "\n",
    "    # Simpan Model Final\n",
    "    torch.save(model_final.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Model Final tersimpan: {MODEL_SAVE_PATH}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 5. Visualization & Evaluation\n",
    "if __name__ == '__main__':\n",
    "    print(f\"\\n=== VISUALISASI & EVALUASI ===\")\n",
    "    \n",
    "    # 1. Gabungkan History\n",
    "    # Catatan: history_extract mungkin tidak ada jika Anda skip Sel 3 dan langsung load model\n",
    "    train_acc = history_tune.get('train_acc', [])\n",
    "    val_acc = history_tune.get('val_acc', [])\n",
    "    train_loss = history_tune.get('train_loss', [])\n",
    "    val_loss = history_tune.get('val_loss', [])\n",
    "\n",
    "    if 'history_extract' in locals():\n",
    "        train_acc = history_extract.get('train_acc', []) + train_acc\n",
    "        val_acc = history_extract.get('val_acc', []) + val_acc\n",
    "        train_loss = history_extract.get('train_loss', []) + train_loss\n",
    "        val_loss = history_extract.get('val_loss', []) + val_loss\n",
    "\n",
    "    # 2. Plotting\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Grafik Akurasi\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_acc, label='Train Acc')\n",
    "    plt.plot(val_acc, label='Val Acc', marker='o') # Marker untuk titik validasi\n",
    "    plt.title('Accuracy History')\n",
    "    plt.xlabel('Epoch (Accumulated)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Grafik Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss', marker='o')\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epoch (Accumulated)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_FILENAME)\n",
    "    plt.show()\n",
    "    print(f\"Plot tersimpan di: {PLOT_FILENAME}\")\n",
    "\n",
    "    # 3. Confusion Matrix\n",
    "    print(f\"\\nMenghitung Confusion Matrix...\")\n",
    "    if 'model_final' in locals():\n",
    "        model_final.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(dataloaders_tune['val'], desc=\"Evaluasi Final\"):\n",
    "                # Optimasi saat Evaluasi\n",
    "                inputs = inputs.to(DEVICE, memory_format=torch.channels_last)\n",
    "                labels = labels.to(DEVICE)\n",
    "                \n",
    "                with torch.autocast(device_type=DEVICE.type, dtype=torch.bfloat16, enabled=True):\n",
    "                    outputs = model_final(inputs)\n",
    "                    \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Plot Heatmap\n",
    "        cf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix (Validation Data)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(CONFUSION_MATRIX_FILENAME)\n",
    "        plt.show()\n",
    "        print(f\"Matrix tersimpan di: {CONFUSION_MATRIX_FILENAME}\")\n",
    "    else:\n",
    "        print(\"Model final tidak ditemukan di memori.\")\n",
    "\n",
    "    print(\"\\n--- PROSES SELESAI ---\")\n",
    "# endregion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
